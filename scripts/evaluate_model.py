#!/usr/bin/env python3
# scripts/evaluate_model.py
"""
Script d'√©valuation d'un mod√®le ML.

G√©n√®re:
- Courbe ROC
- Courbe Precision-Recall
- Feature importance plot
- Courbe de calibration
- Distribution des probabilit√©s
- Analyse par seuil
- Rapport HTML complet

Usage:
    # √âvaluer sur un dataset de test
    python scripts/evaluate_model.py \\
        --model models/saved/xgb_model_latest.joblib \\
        --dataset datasets/test_dataset.parquet \\
        --output reports/evaluation/
    
    # Avec options
    python scripts/evaluate_model.py \\
        --model models/saved/xgb_model_latest.joblib \\
        --dataset datasets/test_dataset.parquet \\
        --output reports/evaluation/ \\
        --name "XGBoost v1.0" \\
        --no-html
"""

import argparse
import sys
from pathlib import Path
from datetime import datetime

# Ajouter le r√©pertoire racine au path
sys.path.insert(0, str(Path(__file__).parent.parent))

from cryptoscalper.utils.logger import setup_logger, logger


def parse_args() -> argparse.Namespace:
    """Parse les arguments de la ligne de commande."""
    parser = argparse.ArgumentParser(
        description="√âvalue un mod√®le ML et g√©n√®re des rapports de visualisation.",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Exemples:
    # √âvaluation basique
    python scripts/evaluate_model.py --model model.joblib --dataset test.parquet
    
    # Avec dossier de sortie personnalis√©
    python scripts/evaluate_model.py --model model.joblib --dataset test.parquet --output reports/
    
    # Sans g√©n√©ration HTML
    python scripts/evaluate_model.py --model model.joblib --dataset test.parquet --no-html
        """
    )
    
    # Arguments obligatoires
    parser.add_argument(
        "--model", "-m",
        type=str,
        required=True,
        help="Chemin du mod√®le (.joblib)"
    )
    
    parser.add_argument(
        "--dataset", "-d",
        type=str,
        required=True,
        help="Chemin du dataset de test (.parquet)"
    )
    
    # Arguments optionnels
    parser.add_argument(
        "--output", "-o",
        type=str,
        default=None,
        help="Dossier de sortie (d√©faut: reports/evaluation_YYYYMMDD_HHMMSS/)"
    )
    
    parser.add_argument(
        "--name", "-n",
        type=str,
        default="XGBoost Classifier",
        help="Nom du mod√®le pour le rapport (d√©faut: 'XGBoost Classifier')"
    )
    
    parser.add_argument(
        "--no-html",
        action="store_true",
        help="Ne pas g√©n√©rer le rapport HTML"
    )
    
    parser.add_argument(
        "--no-plots",
        action="store_true",
        help="Ne pas g√©n√©rer les graphiques"
    )
    
    parser.add_argument(
        "--threshold",
        type=float,
        default=0.5,
        help="Seuil pour les m√©triques principales (d√©faut: 0.5)"
    )
    
    parser.add_argument(
        "--verbose", "-v",
        action="store_true",
        help="Mode verbeux"
    )
    
    return parser.parse_args()


def print_metrics_summary(evaluator, threshold: float = 0.5) -> None:
    """Affiche un r√©sum√© des m√©triques dans la console."""
    import numpy as np
    from sklearn.metrics import (
        accuracy_score, precision_score, recall_score, f1_score,
        roc_auc_score, confusion_matrix
    )
    
    y_true = evaluator.y_true
    y_proba = evaluator.y_proba
    y_pred = (y_proba >= threshold).astype(int)
    
    # M√©triques
    accuracy = accuracy_score(y_true, y_pred)
    precision = precision_score(y_true, y_pred, zero_division=0)
    recall = recall_score(y_true, y_pred, zero_division=0)
    f1 = f1_score(y_true, y_pred, zero_division=0)
    roc_auc = roc_auc_score(y_true, y_proba)
    pr_auc = evaluator.compute_pr_auc()
    brier = evaluator.compute_brier_score()
    
    cm = confusion_matrix(y_true, y_pred)
    
    print("\n" + "=" * 70)
    print("üìä R√âSUM√â DE L'√âVALUATION")
    print("=" * 70)
    
    print(f"\nüìà M√âTRIQUES PRINCIPALES (seuil = {threshold}):")
    print("-" * 50)
    print(f"   ‚Ä¢ ROC-AUC:        {roc_auc:.4f}")
    print(f"   ‚Ä¢ PR-AUC:         {pr_auc:.4f}")
    print(f"   ‚Ä¢ Brier Score:    {brier:.4f}")
    print(f"   ‚Ä¢ Accuracy:       {accuracy:.4f}")
    print(f"   ‚Ä¢ Precision:      {precision:.4f}")
    print(f"   ‚Ä¢ Recall:         {recall:.4f}")
    print(f"   ‚Ä¢ F1-Score:       {f1:.4f}")
    
    print(f"\nüìã MATRICE DE CONFUSION:")
    print("-" * 50)
    print(f"                     Pr√©dit 0    Pr√©dit 1")
    print(f"   Vrai 0 (Neg):     {cm[0][0]:8,}    {cm[0][1]:8,}")
    print(f"   Vrai 1 (Pos):     {cm[1][0]:8,}    {cm[1][1]:8,}")
    
    n_samples = len(y_true)
    n_pos = int(np.sum(y_true))
    print(f"\n   Total: {n_samples:,} √©chantillons")
    print(f"   Positifs: {n_pos:,} ({n_pos/n_samples*100:.1f}%)")
    print(f"   N√©gatifs: {n_samples - n_pos:,} ({(n_samples-n_pos)/n_samples*100:.1f}%)")
    
    # Seuil optimal
    optimal_thresh, optimal_metrics = evaluator.find_optimal_threshold()
    print(f"\nüí° SEUIL OPTIMAL (bas√© sur F1):")
    print("-" * 50)
    print(f"   ‚Ä¢ Seuil:     {optimal_thresh:.2f}")
    print(f"   ‚Ä¢ Precision: {optimal_metrics['precision']:.4f}")
    print(f"   ‚Ä¢ Recall:    {optimal_metrics['recall']:.4f}")
    print(f"   ‚Ä¢ F1:        {optimal_metrics['f1']:.4f}")
    print(f"   ‚Ä¢ Pr√©dictions: {optimal_metrics['n_predictions']:,}")
    
    # Analyse par seuil
    print(f"\nüìâ ANALYSE PAR SEUIL:")
    print("-" * 50)
    print(f"   {'Seuil':<10} {'Precision':<12} {'Recall':<12} {'F1':<12} {'N Preds':<10}")
    
    for thresh in [0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8]:
        y_p = (y_proba >= thresh).astype(int)
        tp = np.sum((y_p == 1) & (y_true == 1))
        fp = np.sum((y_p == 1) & (y_true == 0))
        fn = np.sum((y_p == 0) & (y_true == 1))
        
        prec = tp / (tp + fp) if (tp + fp) > 0 else 0
        rec = tp / (tp + fn) if (tp + fn) > 0 else 0
        f1_t = 2 * prec * rec / (prec + rec) if (prec + rec) > 0 else 0
        n_pred = int(np.sum(y_p))
        
        marker = " ‚Üê optimal" if abs(thresh - optimal_thresh) < 0.01 else ""
        print(f"   {thresh:<10.2f} {prec:<12.4f} {rec:<12.4f} {f1_t:<12.4f} {n_pred:<10,}{marker}")
    
    print("\n" + "=" * 70)


def main() -> int:
    """Point d'entr√©e principal."""
    args = parse_args()
    
    # Configuration du logging
    log_level = "DEBUG" if args.verbose else "INFO"
    setup_logger(level=log_level)
    
    # V√©rifier les fichiers
    model_path = Path(args.model)
    dataset_path = Path(args.dataset)
    
    if not model_path.exists():
        logger.error(f"‚ùå Mod√®le non trouv√©: {model_path}")
        return 1
    
    if not dataset_path.exists():
        logger.error(f"‚ùå Dataset non trouv√©: {dataset_path}")
        return 1
    
    # Dossier de sortie
    if args.output:
        output_dir = Path(args.output)
    else:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_dir = Path(f"reports/evaluation_{timestamp}")
    
    output_dir.mkdir(parents=True, exist_ok=True)
    
    print("\n" + "=" * 70)
    print("üî¨ CryptoScalper AI - √âvaluation du Mod√®le")
    print("=" * 70)
    print(f"\nüìÇ Mod√®le:  {model_path}")
    print(f"üìÇ Dataset: {dataset_path}")
    print(f"üìÅ Sortie:  {output_dir}")
    print(f"üìä Nom:     {args.name}")
    
    try:
        import joblib
        from cryptoscalper.data.dataset import PreparedDataset
        from cryptoscalper.models.evaluator import ModelEvaluator
        
        # Charger le mod√®le
        logger.info(f"üìÇ Chargement du mod√®le...")
        model = joblib.load(model_path)
        
        # Charger le dataset
        logger.info(f"üìÇ Chargement du dataset...")
        dataset = PreparedDataset.load(dataset_path)
        logger.info(f"   {len(dataset):,} √©chantillons, {len(dataset.feature_names)} features")
        
        # Cr√©er l'√©valuateur
        evaluator = ModelEvaluator(model, dataset)
        
        # Afficher le r√©sum√© console
        print_metrics_summary(evaluator, args.threshold)
        
        # G√©n√©rer les graphiques si demand√©
        if not args.no_plots:
            logger.info(f"\nüìä G√©n√©ration des graphiques...")
            plots = evaluator.generate_all_plots(output_dir)
            
            print(f"\nüìä GRAPHIQUES G√âN√âR√âS:")
            print("-" * 50)
            for name, path in plots.items():
                print(f"   ‚úÖ {name}: {path}")
        
        # G√©n√©rer le rapport HTML si demand√©
        if not args.no_html:
            logger.info(f"\nüìÑ G√©n√©ration du rapport HTML...")
            html_path = output_dir / "evaluation_report.html"
            
            # Charger les m√©tadonn√©es du mod√®le si disponibles
            metadata_path = model_path.with_suffix('.json')
            training_result = None
            
            if metadata_path.exists():
                import json
                with open(metadata_path) as f:
                    metadata = json.load(f)
                logger.info(f"   M√©tadonn√©es charg√©es: {metadata_path}")
            
            evaluator.generate_html_report(
                html_path,
                plots=plots if not args.no_plots else None,
                model_name=args.name
            )
            
            print(f"\nüìÑ RAPPORT HTML:")
            print("-" * 50)
            print(f"   ‚úÖ {html_path}")
            print(f"\n   Ouvrir dans le navigateur:")
            print(f"   open {html_path}")
        
        # Sauvegarder les m√©triques en JSON
        import json
        import numpy as np
        
        metrics_dict = {
            "model_path": str(model_path),
            "dataset_path": str(dataset_path),
            "evaluated_at": datetime.now().isoformat(),
            "n_samples": len(dataset),
            "n_features": len(dataset.feature_names),
            "metrics": {
                "roc_auc": float(evaluator.compute_pr_auc()),
                "pr_auc": float(evaluator.compute_pr_auc()),
                "brier_score": float(evaluator.compute_brier_score()),
            },
            "optimal_threshold": float(evaluator.find_optimal_threshold()[0]),
        }
        
        metrics_path = output_dir / "metrics.json"
        with open(metrics_path, 'w') as f:
            json.dump(metrics_dict, f, indent=2)
        
        logger.info(f"üíæ M√©triques sauvegard√©es: {metrics_path}")
        
        print("\n" + "=" * 70)
        print("‚úÖ √âVALUATION TERMIN√âE")
        print("=" * 70)
        print(f"\nüìÅ Tous les fichiers dans: {output_dir}")
        print(f"\n   Fichiers g√©n√©r√©s:")
        for f in sorted(output_dir.iterdir()):
            size_kb = f.stat().st_size / 1024
            print(f"   ‚Ä¢ {f.name} ({size_kb:.1f} KB)")
        
        return 0
        
    except Exception as e:
        logger.error(f"‚ùå Erreur: {e}")
        import traceback
        traceback.print_exc()
        return 1


if __name__ == "__main__":
    sys.exit(main())