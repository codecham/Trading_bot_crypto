#!/usr/bin/env python3
# scripts/train_model.py
"""
Script d'entra√Ænement du mod√®le XGBoost.

Usage:
    # Entra√Æner avec un dataset pr√©par√©
    python scripts/train_model.py --dataset datasets/dataset.parquet
    
    # Entra√Æner avec les splits d√©j√† faits
    python scripts/train_model.py --train datasets/train.parquet \
                                  --val datasets/val.parquet \
                                  --test datasets/test.parquet
    
    # Avec param√®tres personnalis√©s
    python scripts/train_model.py --dataset datasets/dataset.parquet \
                                  --n-estimators 300 \
                                  --max-depth 8 \
                                  --learning-rate 0.03 \
                                  --no-calibrate
    
    # Sauvegarder dans un chemin sp√©cifique
    python scripts/train_model.py --dataset datasets/dataset.parquet \
                                  --output models/saved/my_model.joblib
"""

import argparse
import sys
from pathlib import Path

# Ajouter le r√©pertoire parent au path pour les imports
sys.path.insert(0, str(Path(__file__).parent.parent))

from cryptoscalper.models.trainer import (
    ModelTrainer,
    XGBoostConfig,
    TrainingResult,
    print_threshold_analysis,
    find_optimal_threshold,
)
from cryptoscalper.data.dataset import PreparedDataset, LabelConfig
from cryptoscalper.utils.logger import setup_logger, logger


def parse_args() -> argparse.Namespace:
    """Parse les arguments de ligne de commande."""
    parser = argparse.ArgumentParser(
        description="Entra√Æne un mod√®le XGBoost pour CryptoScalper",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Exemples:
  # Entra√Æner avec un dataset unique (sera splitt√© automatiquement)
  python scripts/train_model.py --dataset datasets/dataset.parquet
  
  # Entra√Æner avec des splits pr√©-calcul√©s
  python scripts/train_model.py --train datasets/train.parquet \\
                                --val datasets/val.parquet \\
                                --test datasets/test.parquet
  
  # Personnaliser les hyperparam√®tres
  python scripts/train_model.py --dataset datasets/dataset.parquet \\
                                --n-estimators 300 \\
                                --max-depth 8 \\
                                --learning-rate 0.03
        """
    )
    
    # === Donn√©es ===
    data_group = parser.add_argument_group("Donn√©es")
    data_group.add_argument(
        "--dataset",
        type=str,
        help="Chemin vers le dataset complet (sera splitt√© 70/15/15)"
    )
    data_group.add_argument(
        "--train",
        type=str,
        help="Chemin vers le dataset d'entra√Ænement"
    )
    data_group.add_argument(
        "--val",
        type=str,
        help="Chemin vers le dataset de validation"
    )
    data_group.add_argument(
        "--test",
        type=str,
        help="Chemin vers le dataset de test (optionnel)"
    )
    
    # === Hyperparam√®tres XGBoost ===
    xgb_group = parser.add_argument_group("Hyperparam√®tres XGBoost")
    xgb_group.add_argument(
        "--n-estimators",
        type=int,
        default=200,
        help="Nombre d'arbres (default: 200)"
    )
    xgb_group.add_argument(
        "--max-depth",
        type=int,
        default=6,
        help="Profondeur max des arbres (default: 6)"
    )
    xgb_group.add_argument(
        "--learning-rate",
        type=float,
        default=0.05,
        help="Taux d'apprentissage (default: 0.05)"
    )
    xgb_group.add_argument(
        "--subsample",
        type=float,
        default=0.8,
        help="Ratio de sous-√©chantillonnage (default: 0.8)"
    )
    xgb_group.add_argument(
        "--colsample-bytree",
        type=float,
        default=0.8,
        help="Ratio de colonnes par arbre (default: 0.8)"
    )
    xgb_group.add_argument(
        "--scale-pos-weight",
        type=float,
        default=None,
        help="Poids pour la classe positive (auto si non sp√©cifi√©)"
    )
    xgb_group.add_argument(
        "--early-stopping",
        type=int,
        default=20,
        help="Early stopping rounds (default: 20)"
    )
    
    # === Calibration ===
    cal_group = parser.add_argument_group("Calibration")
    cal_group.add_argument(
        "--no-calibrate",
        action="store_true",
        help="D√©sactiver la calibration des probabilit√©s"
    )
    cal_group.add_argument(
        "--calibration-method",
        type=str,
        default="isotonic",
        choices=["isotonic", "sigmoid"],
        help="M√©thode de calibration (default: isotonic)"
    )
    cal_group.add_argument(
        "--calibration-cv",
        type=int,
        default=5,
        help="CV folds pour la calibration (default: 5)"
    )
    
    # === Sortie ===
    out_group = parser.add_argument_group("Sortie")
    out_group.add_argument(
        "--output",
        type=str,
        default="models/saved/xgb_model_latest.joblib",
        help="Chemin de sauvegarde du mod√®le"
    )
    out_group.add_argument(
        "--no-save",
        action="store_true",
        help="Ne pas sauvegarder le mod√®le"
    )
    
    # === Options ===
    parser.add_argument(
        "-v", "--verbose",
        action="store_true",
        help="Mode verbose"
    )
    parser.add_argument(
        "-q", "--quiet",
        action="store_true",
        help="Mode silencieux (pas de logs XGBoost)"
    )
    
    return parser.parse_args()


def load_datasets(args: argparse.Namespace) -> tuple:
    """
    Charge les datasets selon les arguments.
    
    Returns:
        (train, val, test) - test peut √™tre None
    """
    if args.dataset:
        # Un seul fichier ‚Üí split automatique
        logger.info(f"üìÇ Chargement du dataset: {args.dataset}")
        dataset = PreparedDataset.load(Path(args.dataset))
        
        logger.info(f"   {len(dataset):,} lignes, {dataset.stats.feature_count} features")
        logger.info(f"   Labels: {dataset.stats.label_ratio:.1%} positifs")
        
        # Split
        logger.info("üìä Split temporel 70/15/15...")
        train, val, test = dataset.split_temporal()
        
        return train, val, test
    
    elif args.train and args.val:
        # Fichiers s√©par√©s
        logger.info(f"üìÇ Chargement train: {args.train}")
        train = PreparedDataset.load(Path(args.train))
        
        logger.info(f"üìÇ Chargement val: {args.val}")
        val = PreparedDataset.load(Path(args.val))
        
        test = None
        if args.test:
            logger.info(f"üìÇ Chargement test: {args.test}")
            test = PreparedDataset.load(Path(args.test))
        
        return train, val, test
    
    else:
        raise ValueError(
            "Sp√©cifiez --dataset OU (--train et --val). "
            "Utilisez --help pour plus d'infos."
        )


def create_config(args: argparse.Namespace) -> XGBoostConfig:
    """Cr√©e la configuration XGBoost depuis les arguments."""
    return XGBoostConfig(
        n_estimators=args.n_estimators,
        max_depth=args.max_depth,
        learning_rate=args.learning_rate,
        subsample=args.subsample,
        colsample_bytree=args.colsample_bytree,
        scale_pos_weight=args.scale_pos_weight,
        early_stopping_rounds=args.early_stopping,
        calibrate=not args.no_calibrate,
        calibration_method=args.calibration_method,
        calibration_cv=args.calibration_cv,
    )


def print_summary(result: TrainingResult) -> None:
    """Affiche un r√©sum√© complet de l'entra√Ænement."""
    print("\n" + "=" * 70)
    print("üéØ R√âSUM√â DE L'ENTRA√éNEMENT")
    print("=" * 70)
    
    print(f"\n‚è±Ô∏è  Temps d'entra√Ænement: {result.training_time_seconds:.1f}s")
    print(f"üå≥ Meilleure it√©ration: {result.best_iteration}")
    print(f"üìê Mod√®le calibr√©: {'Oui ‚úÖ' if result.is_calibrated else 'Non'}")
    
    # M√©triques validation
    print("\nüìä M√âTRIQUES VALIDATION:")
    print("-" * 40)
    print(f"   Accuracy:  {result.val_metrics.accuracy:.4f}")
    print(f"   Precision: {result.val_metrics.precision:.4f}")
    print(f"   Recall:    {result.val_metrics.recall:.4f}")
    print(f"   F1-Score:  {result.val_metrics.f1:.4f}")
    print(f"   ROC-AUC:   {result.val_metrics.roc_auc:.4f}")
    
    # Matrice de confusion
    cm = result.val_metrics.confusion_matrix
    print("\n   Matrice de confusion:")
    print(f"                 Pr√©dit 0    Pr√©dit 1")
    print(f"   Vrai 0:       {cm[0][0]:8.0f}    {cm[0][1]:8.0f}")
    print(f"   Vrai 1:       {cm[1][0]:8.0f}    {cm[1][1]:8.0f}")
    
    # M√©triques test si disponibles
    if result.test_metrics:
        print("\nüìä M√âTRIQUES TEST:")
        print("-" * 40)
        print(f"   Accuracy:  {result.test_metrics.accuracy:.4f}")
        print(f"   Precision: {result.test_metrics.precision:.4f}")
        print(f"   Recall:    {result.test_metrics.recall:.4f}")
        print(f"   F1-Score:  {result.test_metrics.f1:.4f}")
        print(f"   ROC-AUC:   {result.test_metrics.roc_auc:.4f}")
    
    # Feature importance
    if result.feature_importance:
        print("\nüèÜ TOP 15 FEATURES:")
        print("-" * 40)
        for i, (name, score) in enumerate(result.feature_importance.top_features[:15], 1):
            bar = "‚ñà" * int(score * 40)
            print(f"   {i:2d}. {name:28s} {score:.4f} {bar}")
    
    # Analyse par seuil
    print_threshold_analysis(result.val_metrics)
    
    # Seuil recommand√©
    optimal = find_optimal_threshold(result.val_metrics)
    if optimal:
        print(f"\nüí° Seuil recommand√©: {optimal:.2f}")
    
    print("\n" + "=" * 70)


def main() -> int:
    """Point d'entr√©e principal."""
    args = parse_args()
    
    # Setup logging
    log_level = "DEBUG" if args.verbose else "INFO"
    if args.quiet:
        log_level = "WARNING"
    setup_logger(level=log_level)
    
    print("=" * 70)
    print("ü§ñ CryptoScalper AI - Entra√Ænement Mod√®le XGBoost")
    print("=" * 70)
    
    try:
        # Charger les donn√©es
        train_data, val_data, test_data = load_datasets(args)
        
        print(f"\nüìã Donn√©es charg√©es:")
        print(f"   Train: {len(train_data):,} samples ({train_data.stats.label_ratio:.1%} positifs)")
        print(f"   Val:   {len(val_data):,} samples ({val_data.stats.label_ratio:.1%} positifs)")
        if test_data:
            print(f"   Test:  {len(test_data):,} samples ({test_data.stats.label_ratio:.1%} positifs)")
        
        # Cr√©er la configuration
        config = create_config(args)
        
        print(f"\n‚öôÔ∏è  Configuration XGBoost:")
        print(f"   n_estimators:    {config.n_estimators}")
        print(f"   max_depth:       {config.max_depth}")
        print(f"   learning_rate:   {config.learning_rate}")
        print(f"   subsample:       {config.subsample}")
        print(f"   colsample:       {config.colsample_bytree}")
        print(f"   early_stopping:  {config.early_stopping_rounds}")
        print(f"   calibration:     {'Oui' if config.calibrate else 'Non'}")
        
        # Entra√Æner
        print("\n" + "=" * 70)
        print("üöÄ ENTRA√éNEMENT EN COURS...")
        print("=" * 70 + "\n")
        
        trainer = ModelTrainer(config)
        result = trainer.train(
            train_data,
            val_data,
            test_data,
            verbose=not args.quiet
        )
        
        # Afficher le r√©sum√©
        print_summary(result)
        
        # Sauvegarder
        if not args.no_save:
            output_path = Path(args.output)
            paths = trainer.save_training_result(result, output_path.parent)
            
            print(f"\nüíæ FICHIERS SAUVEGARD√âS:")
            for name, path in paths.items():
                print(f"   {name}: {path}")
        
        print("\n‚úÖ Entra√Ænement termin√© avec succ√®s!")
        
        # Afficher un rappel si AUC est faible
        if result.val_metrics.roc_auc < 0.55:
            print("\n‚ö†Ô∏è  ATTENTION: AUC faible (<0.55)")
            print("   Le mod√®le n'est pas meilleur qu'un tirage al√©atoire.")
            print("   Essayez:")
            print("   - Plus de donn√©es d'entra√Ænement")
            print("   - Diff√©rents hyperparam√®tres")
            print("   - Features suppl√©mentaires")
        
        return 0
        
    except FileNotFoundError as e:
        logger.error(f"‚ùå Fichier non trouv√©: {e}")
        return 1
    except ValueError as e:
        logger.error(f"‚ùå Erreur de configuration: {e}")
        return 1
    except Exception as e:
        logger.exception(f"‚ùå Erreur inattendue: {e}")
        return 1


if __name__ == "__main__":
    sys.exit(main())