# scripts/prepare_dataset.py
"""
Script pour pr√©parer le dataset d'entra√Ænement.

Usage:
    # Pr√©parer un dataset depuis les donn√©es t√©l√©charg√©es
    python scripts/prepare_dataset.py --symbols BTCUSDT,ETHUSDT --output datasets/train_dataset.parquet
    
    # Avec configuration personnalis√©e
    python scripts/prepare_dataset.py --symbols BTCUSDT --horizon 5 --threshold 0.003
"""

import argparse
import sys
from pathlib import Path

# Ajouter le dossier parent au path
sys.path.insert(0, str(Path(__file__).parent.parent))

from cryptoscalper.utils.logger import setup_logger, logger
from cryptoscalper.data.dataset import (
    DatasetBuilder,
    LabelConfig,
    SplitConfig,
    prepare_dataset,
    analyze_class_balance,
)
from cryptoscalper.data.historical import is_data_cached


# Symboles par d√©faut
DEFAULT_SYMBOLS = ["BTCUSDT", "ETHUSDT", "BNBUSDT"]


def parse_args():
    """Parse les arguments de ligne de commande."""
    parser = argparse.ArgumentParser(
        description="Pr√©pare le dataset pour l'entra√Ænement ML"
    )
    
    # Symboles
    parser.add_argument(
        "--symbols",
        type=str,
        required=True,
        help="Liste de symboles s√©par√©s par des virgules"
    )
    
    # Configuration des labels
    parser.add_argument(
        "--horizon",
        type=int,
        default=3,
        help="Horizon de pr√©diction en minutes (d√©faut: 3)"
    )
    parser.add_argument(
        "--threshold",
        type=float,
        default=0.002,
        help="Seuil de hausse pour label=1 (d√©faut: 0.002 = 0.2%%)"
    )
    
    # Sortie
    parser.add_argument(
        "--output",
        type=str,
        default="datasets/prepared_dataset.parquet",
        help="Fichier de sortie"
    )
    parser.add_argument(
        "--data-dir",
        type=str,
        default="data_cache",
        help="Dossier des donn√©es sources"
    )
    
    # Options
    parser.add_argument(
        "--split",
        action="store_true",
        help="Sauvegarder aussi les splits train/val/test"
    )
    parser.add_argument(
        "--verbose", "-v",
        action="store_true",
        help="Mode verbose"
    )
    
    return parser.parse_args()


def main():
    """Point d'entr√©e principal."""
    args = parse_args()
    
    # Setup logging
    log_level = "DEBUG" if args.verbose else "INFO"
    setup_logger(level=log_level)
    
    print("=" * 60)
    print("üîß CryptoScalper AI - Pr√©paration Dataset")
    print("=" * 60)
    
    # Parser les symboles
    symbols = [s.strip().upper() for s in args.symbols.split(",")]
    
    # V√©rifier que les donn√©es existent
    data_dir = Path(args.data_dir)
    missing = [s for s in symbols if not is_data_cached(s, data_dir)]
    if missing:
        logger.error(f"‚ùå Donn√©es manquantes pour: {missing}")
        logger.info("Lancez d'abord: python scripts/download_data.py --symbols ...")
        return 1
    
    # Configuration
    label_config = LabelConfig(
        horizon_minutes=args.horizon,
        threshold_percent=args.threshold
    )
    
    print(f"\nüìã Configuration:")
    print(f"   Symboles: {', '.join(symbols)}")
    print(f"   Horizon: {args.horizon} minutes")
    print(f"   Seuil: {args.threshold:.2%}")
    print(f"   Sortie: {args.output}")
    print()
    
    # Construire le dataset
    builder = DatasetBuilder(label_config=label_config)
    
    try:
        dataset = builder.build_from_symbols(symbols, data_dir)
    except Exception as e:
        logger.error(f"‚ùå Erreur construction dataset: {e}")
        return 1
    
    # Afficher les stats
    print("\n" + "=" * 60)
    print("üìä STATISTIQUES DU DATASET")
    print("=" * 60)
    print(dataset.stats.summary())
    
    # Analyse de l'√©quilibre
    balance = analyze_class_balance(dataset.labels)
    print(f"\nüìà √âquilibre des classes:")
    print(f"   Ratio positifs: {balance['positive_ratio']:.1%}")
    print(f"   Ratio n√©gatifs: {balance['negative_ratio']:.1%}")
    print(f"   D√©s√©quilibre: {balance['imbalance_ratio']:.2f}x")
    
    # Sauvegarder
    output_path = Path(args.output)
    output_path.parent.mkdir(parents=True, exist_ok=True)
    dataset.save(output_path)
    
    # Split si demand√©
    if args.split:
        print("\nüìÇ Sauvegarde des splits...")
        train, val, test = dataset.split_temporal()
        
        base_name = output_path.stem
        train.save(output_path.parent / f"{base_name}_train.parquet")
        val.save(output_path.parent / f"{base_name}_val.parquet")
        test.save(output_path.parent / f"{base_name}_test.parquet")
    
    print("\n" + "=" * 60)
    print("‚úÖ DATASET PR√âPAR√â AVEC SUCC√àS")
    print("=" * 60)
    print(f"   Fichier: {args.output}")
    print(f"   Lignes: {len(dataset):,}")
    print(f"   Features: {dataset.stats.feature_count}")
    
    return 0


if __name__ == "__main__":
    sys.exit(main())